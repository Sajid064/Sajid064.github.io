---
title: "SemanticCuetSync at ArAIEval Shared Task: Detecting Propagandistic Spans with Persuasion Techniques Identification using Pre-trained Transformers
"
collection: publications
permalink: /publication/3
excerpt: ''
date: 2024-08-10
venue: 'In Proceedings of The Second Arabic Natural Language Processing Conference, pages 518–523, Bangkok, Thailand. Association for Computational Linguistics.
'
paperurl: 'https://aclanthology.org/2024.arabicnlp-1.54/'
citation: 'Symom Shohan, Md. Hossain, Ashraful Paran, Shawly Ahsan, Jawad Hossain, and Mohammed Moshiul Hoque. 2024. SemanticCuetSync at ArAIEval Shared Task: Detecting Propagandistic Spans with Persuasion Techniques Identification using Pre-trained Transformers. In Proceedings of The Second Arabic Natural Language Processing Conference, pages 518–523, Bangkok, Thailand. Association for Computational Linguistics.'
---

Detecting propagandistic spans and identifying persuasion techniques are crucial for promoting informed decision-making, safeguarding democratic processes, and fostering a media environment characterized by integrity and transparency. Various machine learning (Logistic Regression, Random Forest, and Multinomial Naive Bayes), deep learning (CNN, CNN+LSTM, CNN+BiLSTM), and transformer-based (AraBERTv2, AraBERT-NER, CamelBERT, BERT-Base-Arabic) models were exploited to perform the task. The evaluation results indicate that CamelBERT achieved the highest micro-F1 score (24.09%), outperforming CNN+LSTM and AraBERTv2. The study found that most models struggle to detect propagandistic spans when multiple spans are present within the same article. Overall, the model’s performance secured a 6th place ranking in the ArAIEval Shared Task-1.